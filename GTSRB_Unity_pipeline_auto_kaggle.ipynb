{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed4eacc",
   "metadata": {},
   "source": [
    "# GTSRB Traffic Sign Classifier → ONNX → Unity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa07a67",
   "metadata": {},
   "source": [
    "##  Requirements\n",
    "Run this cell to install required packages (skip if already installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m pip install --upgrade pip\n",
    "python -m pip install torch torchvision onnx onnxruntime opencv-python pillow numpy kaggle tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6db531",
   "metadata": {},
   "source": [
    "## 1 — Download real GTSRB from Kaggle if possible, else create synthetic dataset\n",
    "This cell checks for `~/.kaggle/kaggle.json`. If present it will attempt to download a public GTSRB mirror via the Kaggle CLI. If the download succeeds, it prepares the ImageFolder layout. If not, it creates a small synthetic ImageFolder dataset (4 classes) so you can run training immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bfccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, subprocess, sys, shutil, random\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path('data/gtsrb_imagefolder')\n",
    "KAGGLE_TOKEN = Path.home() / '.kaggle' / 'kaggle.json'\n",
    "print('Checking for Kaggle token at', KAGGLE_TOKEN)\n",
    "download_success = False\n",
    "if KAGGLE_TOKEN.exists():\n",
    "    print('Kaggle token found. Attempting to download GTSRB dataset via Kaggle CLI (public mirror).')\n",
    "    os.makedirs('data/gtsrb', exist_ok=True)\n",
    "    try:\n",
    "        cmd = ['kaggle', 'datasets', 'download', '-d', 'ibrahimkaratas/gtsrb-german-traffic-sign-recognition-benchmark', '-p', 'data/gtsrb', '--unzip']\n",
    "        print('Running:', ' '.join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "        download_success = True\n",
    "        print('Downloaded GTSRB mirror. Preparing ImageFolder structure...')\n",
    "        src_candidates = [Path('data/gtsrb'), Path('data/gtsrb/GTSRB'), Path('data/gtsrb/Final_Training/Images')]\n",
    "        src = None\n",
    "        for cand in src_candidates:\n",
    "            if cand.exists():\n",
    "                # look for class subdirectories\n",
    "                for p in cand.iterdir():\n",
    "                    if p.is_dir():\n",
    "                        src = cand\n",
    "                        break\n",
    "                if src:\n",
    "                    break\n",
    "        if src is None:\n",
    "            print('Downloaded content not found in expected locations. Will fallback to synthetic dataset.')\n",
    "            download_success = False\n",
    "        else:\n",
    "            if DATA_ROOT.exists():\n",
    "                shutil.rmtree(DATA_ROOT)\n",
    "            DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "            for cls_dir in src.iterdir():\n",
    "                if cls_dir.is_dir():\n",
    "                    imgs = list(cls_dir.glob('*.*'))\n",
    "                    if not imgs:\n",
    "                        continue\n",
    "                    random.shuffle(imgs)\n",
    "                    split = int(len(imgs) * 0.8)\n",
    "                    train = imgs[:split]; val = imgs[split:]\n",
    "                    for t in train:\n",
    "                        dst = DATA_ROOT / 'train' / cls_dir.name\n",
    "                        dst.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.copy(t, dst / t.name)\n",
    "                    for v in val:\n",
    "                        dst = DATA_ROOT / 'val' / cls_dir.name\n",
    "                        dst.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.copy(v, dst / v.name)\n",
    "            print('Prepared ImageFolder at', DATA_ROOT)\n",
    "    except Exception as e:\n",
    "        print('Kaggle download or preparation failed:', e)\n",
    "        download_success = False\n",
    "\n",
    "if not download_success:\n",
    "    print('Creating synthetic fallback dataset (small) at', DATA_ROOT)\n",
    "    from PIL import Image, ImageDraw\n",
    "    classes = ['left','right','forward','stop']\n",
    "    n_per_class = 40\n",
    "    sz = 128\n",
    "    if DATA_ROOT.exists():\n",
    "        shutil.rmtree(DATA_ROOT)\n",
    "    for cls in classes:\n",
    "        d = DATA_ROOT / 'train' / cls\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        for i in range(n_per_class):\n",
    "            img = Image.new('RGB', (sz, sz), color=(255,255,255))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            color = tuple([int(x) for x in (random.randint(50,220), random.randint(50,220), random.randint(50,220))])\n",
    "            r = random.randint(20,48)\n",
    "            cx = random.randint(r, sz-r)\n",
    "            cy = random.randint(r, sz-r)\n",
    "            draw.ellipse((cx-r, cy-r, cx+r, cy+r), fill=color, outline=(0,0,0))\n",
    "            if cls == 'left':\n",
    "                draw.polygon([(cx- r//2, cy), (cx + r//2, cy - r//2), (cx + r//2, cy + r//2)], fill=(0,0,0))\n",
    "            elif cls == 'right':\n",
    "                draw.polygon([(cx + r//2, cy), (cx - r//2, cy - r//2), (cx - r//2, cy + r//2)], fill=(0,0,0))\n",
    "            elif cls == 'forward':\n",
    "                draw.polygon([(cx, cy - r//2), (cx - r//2, cy + r//2), (cx + r//2, cy + r//2)], fill=(0,0,0))\n",
    "            else:\n",
    "                draw.rectangle((cx - r//2, cy - r//2, cx + r//2, cy + r//2), fill=(0,0,0))\n",
    "            img.save(d / f'{cls}_{i}.png')\n",
    "    # small val split\n",
    "    for cls in classes:\n",
    "        src = DATA_ROOT / 'train' / cls\n",
    "        dst = DATA_ROOT / 'val' / cls\n",
    "        dst.mkdir(parents=True, exist_ok=True)\n",
    "        files = list(src.iterdir())\n",
    "        random.shuffle(files)\n",
    "        take = files[:8]\n",
    "        for f in take:\n",
    "            f.rename(dst / f.name)\n",
    "    print('Synthetic dataset created at', DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d163f",
   "metadata": {},
   "source": [
    "## 2 — Train (on real GTSRB if downloaded, else synthetic) and export ONNX\n",
    "This cell will run training on whatever `data/gtsrb_imagefolder` currently contains (real or synthetic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, os\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATA_DIR = 'data/gtsrb_imagefolder'\n",
    "EPOCHS = 6\n",
    "BATCH = 64\n",
    "LR = 3e-4\n",
    "OUT_ONNX_REAL = 'gtsrb.onnx'\n",
    "OUT_ONNX_SYN = 'gtsrb_synthetic.onnx'\n",
    "OUT_PTH = 'gtsrb_model.pth'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise RuntimeError('Data folder not found. Run the download/prep cell first.')\n",
    "tr = transforms.Compose([transforms.Resize((128,128)),\n",
    "                         transforms.RandomRotation(10),\n",
    "                         transforms.ColorJitter(0.1,0.1,0.1),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "valtr = transforms.Compose([transforms.Resize((128,128)),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "train_ds = datasets.ImageFolder(os.path.join(DATA_DIR,'train'), transform=tr)\n",
    "val_ds = datasets.ImageFolder(os.path.join(DATA_DIR,'val'), transform=valtr)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "print('Num classes:', len(train_ds.classes), train_ds.classes)\n",
    "\n",
    "# Use smaller model settings for speed if dataset is synthetic (small)\n",
    "use_synthetic = (len(train_ds) < 2000)\n",
    "model = models.resnet18(pretrained=not use_synthetic)  # if synthetic keep training from scratch to avoid download/pretrained deps\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))\n",
    "model = model.to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for X,y in train_loader:\n",
    "        X=X.to(device); y=y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = crit(out,y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running += loss.item() * X.size(0)\n",
    "    train_loss = running / len(train_ds)\n",
    "    model.eval()\n",
    "    correct=0; total=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in val_loader:\n",
    "            X=X.to(device); y=y.to(device)\n",
    "            out = model(X)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total += y.size(0)\n",
    "    val_acc = correct/total if total>0 else 0.0\n",
    "    print(f'Epoch {e+1}/{EPOCHS} train_loss={train_loss:.4f} val_acc={val_acc:.4f}')\n",
    "\n",
    "# save checkpoint\n",
    "torch.save({'model':model.state_dict(),'classes':train_ds.classes}, OUT_PTH)\n",
    "print('Saved checkpoint', OUT_PTH)\n",
    "\n",
    "# export ONNX\n",
    "model.eval()\n",
    "dummy = torch.randn(1,3,128,128, device=device)\n",
    "out_name = OUT_ONNX_REAL if not use_synthetic else OUT_ONNX_SYN\n",
    "torch.onnx.export(model, dummy, out_name, input_names=['input'], output_names=['output'], opset_version=11)\n",
    "print('Exported ONNX to', out_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bdda70",
   "metadata": {},
   "source": [
    "## 3 — ONNX runtime check (optional)\n",
    "This runs a quick inference using ONNXRuntime on one image from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc05cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import onnxruntime as rt\n",
    "    from PIL import Image\n",
    "    import numpy as np, os\n",
    "    onnx_file = 'gtsrb.onnx' if os.path.exists('gtsrb.onnx') else 'gtsrb_synthetic.onnx'\n",
    "    print('Using ONNX file:', onnx_file)\n",
    "    sess = rt.InferenceSession(onnx_file)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    img_path = None\n",
    "    for root,dirs,files in os.walk('data/gtsrb_imagefolder/val'):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.png','.jpg','.ppm','.jpeg')):\n",
    "                img_path = os.path.join(root,f); break\n",
    "        if img_path: break\n",
    "    if img_path is None:\n",
    "        print('No validation image found to test ONNX.')\n",
    "    else:\n",
    "        img = Image.open(img_path).convert('RGB').resize((128,128))\n",
    "        arr = np.array(img).astype(np.float32)/255.0\n",
    "        mean = np.array([0.485,0.456,0.406],dtype=np.float32)\n",
    "        std = np.array([0.229,0.224,0.225],dtype=np.float32)\n",
    "        arr = (arr - mean) / std\n",
    "        arr = arr.transpose(2,0,1)[None,...]\n",
    "        out = sess.run(None, {input_name: arr})\n",
    "        print('ONNX output shape:', out[0].shape)\n",
    "except Exception as e:\n",
    "    print('ONNX runtime test failed (maybe onnxruntime not installed). Error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbe071",
   "metadata": {},
   "source": [
    "## 4 — Unity script\n",
    "The Unity C# script (`WebcamModelController.cs`) was saved to the notebook folder. Path: `/mnt/data/WebcamModelController.cs`. Import it into Unity `Assets/` and point its `classes` array to the trained class names (the order matches `model.classes` saved in the checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unity script saved to:', r'/mnt/data/WebcamModelController.cs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-mobipi]",
   "language": "python",
   "name": "conda-env-Anaconda3-mobipi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
