{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12bcf9c",
   "metadata": {},
   "source": [
    "Hull Tactical — Final GPU-ready Notebook\n",
    "\n",
    "This notebook includes the patched pipeline with sorting, train-only median imputation, lagged-column handling, is_scored support, purged+embargo CV, Optuna tuning, stacking, and GPU support for LightGBM/CatBoost.\n",
    "\n",
    "Instructions:\n",
    "1. (Optional) Install dependencies.\n",
    "2. Edit TRAIN_CSV/TEST_CSV paths in the run cell.\n",
    "3. Run cells top-to-bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d37e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: install required packages in a fresh environment (uncomment to run)\n",
    "#!pip install numpy pandas scikit-learn lightgbm catboost optuna joblib nbformat\n",
    "print('If running on a fresh environment, uncomment and run the pip install line.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, math\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    lgb = None\n",
    "    print('Warning: lightgbm import failed:', e)\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except Exception as e:\n",
    "    CatBoostRegressor = None\n",
    "    print('Warning: catboost import failed:', e)\n",
    "\n",
    "TARGET_COL = 'forward_returns'\n",
    "TIME_COL = 'date_id'\n",
    "DEFAULT_N_SPLITS = 5\n",
    "DEFAULT_PURGE_DAYS = 30\n",
    "DEFAULT_EMBARGO_DAYS = 2\n",
    "DEFAULT_N_TRIALS = 40\n",
    "DEFAULT_TOP_K = 120\n",
    "DEFAULT_TRANSACTION_COST = 0.0003\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print('Environment ready. LGB:', lgb is not None, 'CatBoost:', CatBoostRegressor is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_sharpe(returns, market_returns, rf=0.0):\n",
    "    excess = returns - rf\n",
    "    market_vol = np.std(market_returns)\n",
    "    strat_vol = np.std(excess)\n",
    "    penalty = np.clip(strat_vol / (market_vol + 1e-8), 0.5, 2.0)\n",
    "    ratio = (np.mean(excess) / (strat_vol + 1e-8)) / penalty\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def compute_rsi(series, window=14):\n",
    "    delta = series.diff().fillna(0)\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(span=window, adjust=False).mean()\n",
    "    roll_down = down.ewm(span=window, adjust=False).mean()\n",
    "    rs = roll_up / (roll_down + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "def feature_engineer(df, price_col):\n",
    "    df = df.copy()\n",
    "    df['ret_1'] = df[price_col].pct_change().fillna(0)\n",
    "    for lag in (1,2,3,5,7,10):\n",
    "        df[f'ret_lag_{lag}'] = df['ret_1'].shift(lag).fillna(0)\n",
    "        df[f'price_lag_{lag}'] = df[price_col].shift(lag).fillna(method='bfill')\n",
    "    for w in (3,5,10,20,30,60):\n",
    "        df[f'ma_{w}'] = df[price_col].rolling(window=w, min_periods=1).mean()\n",
    "        df[f'std_{w}'] = df['ret_1'].rolling(window=w, min_periods=1).std().fillna(0)\n",
    "        df[f'rsi_{w}'] = compute_rsi(df[price_col], window=w)\n",
    "        df[f'pctile_{w}'] = df[price_col].rolling(window=w, min_periods=1).apply(lambda x: pd.Series(x).rank(pct=True).iloc[-1])\n",
    "    if 'ma_10' in df.columns and 'ma_30' in df.columns:\n",
    "        df['mom_10_30'] = df['ma_10'] - df['ma_30']\n",
    "    else:\n",
    "        df['mom_10_30'] = df['ma_10'] - df[price_col].rolling(30, min_periods=1).mean()\n",
    "    if 'std_10' in df.columns and 'std_30' in df.columns:\n",
    "        df['vol_ratio_10_30'] = df['std_10'] / (df['std_30'].replace(0,1e-8))\n",
    "    else:\n",
    "        df['vol_ratio_10_30'] = df['std_10'] / (df['ret_1'].rolling(30, min_periods=1).std().replace(0,1e-8))\n",
    "    if 'date' in df.columns:\n",
    "        d = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df['dow'] = d.dt.dayofweek.fillna(-1).astype(int)\n",
    "        df['month'] = d.dt.month.fillna(-1).astype(int)\n",
    "        df['day'] = d.dt.day.fillna(-1).astype(int)\n",
    "    df.fillna(0, inplace=False)\n",
    "    return df\n",
    "\n",
    "print('Feature engineering functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_embargo_splits(n_samples, n_splits=DEFAULT_N_SPLITS, purge=DEFAULT_PURGE_DAYS, embargo=DEFAULT_EMBARGO_DAYS):\n",
    "    idx = np.arange(n_samples)\n",
    "    block_size = max(1, n_samples // n_splits)\n",
    "    for i in range(n_splits):\n",
    "        val_start = i * block_size\n",
    "        val_end = min(n_samples, val_start + block_size)\n",
    "        train_end = max(0, val_start - purge)\n",
    "        train_idx = idx[:train_end]\n",
    "        val_idx = idx[val_start:val_end]\n",
    "        yield train_idx, val_idx\n",
    "\n",
    "\n",
    "def backtest_with_cost_from_preds(preds, realized_returns, top_q=0.2, tc=DEFAULT_TRANSACTION_COST):\n",
    "    q = np.quantile(preds, 1 - top_q)\n",
    "    signal = (preds >= q).astype(int)\n",
    "    pos = np.roll(signal, 1)\n",
    "    pos[0] = 0\n",
    "    turnover = np.abs(np.diff(pos, prepend=0))\n",
    "    cost = turnover * tc\n",
    "    strat_return = pos * realized_returns - cost\n",
    "    return strat_return, pos, cost\n",
    "\n",
    "print('Split and backtest utilities ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y, ret_market, splits, use_gpu):\n",
    "    lgb_params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 127),\n",
    "        'learning_rate': trial.suggest_float('lgb_lr', 0.01, 0.2),\n",
    "        'n_estimators': trial.suggest_int('lgb_estimators', 200, 800),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    cat_iters = trial.suggest_int('cat_iters', 300, 1000)\n",
    "    cat_depth = trial.suggest_int('cat_depth', 4, 8)\n",
    "    cat_lr = trial.suggest_float('cat_lr', 0.01, 0.1)\n",
    "    top_q = trial.suggest_float('top_q', 0.05, 0.5)\n",
    "\n",
    "    preds_all = []\n",
    "    rets_all = []\n",
    "    for train_idx, val_idx in splits:\n",
    "        if len(train_idx) < 10 or len(val_idx) < 10:\n",
    "            continue\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        if lgb is None:\n",
    "            raise RuntimeError('lightgbm not available.')\n",
    "        lgb_args = dict(lgb_params)\n",
    "        if use_gpu:\n",
    "            lgb_args['device'] = 'gpu'\n",
    "            lgb_args['gpu_platform_id'] = 0\n",
    "            lgb_args['gpu_device_id'] = 0\n",
    "        mdl_lgb = lgb.LGBMRegressor(**lgb_args)\n",
    "        try:\n",
    "            mdl_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "        except TypeError:\n",
    "            # older/newer lgb builds might not accept early_stopping_rounds here — fall back to a plain fit\n",
    "            mdl_lgb.fit(X_train, y_train)\n",
    "\n",
    "        if CatBoostRegressor is None:\n",
    "            raise RuntimeError('catboost not available.')\n",
    "        cat_args = {'iterations': cat_iters, 'depth': cat_depth, 'learning_rate': cat_lr, 'verbose': False, 'random_state': RANDOM_STATE}\n",
    "        if use_gpu:\n",
    "            cat_args['task_type'] = 'GPU'\n",
    "        mdl_cat = CatBoostRegressor(**cat_args)\n",
    "        mdl_cat.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True, verbose=False)\n",
    "\n",
    "        pred = 0.5 * mdl_lgb.predict(X_val) + 0.5 * mdl_cat.predict(X_val)\n",
    "        strat_ret, _, _ = backtest_with_cost_from_preds(pred, y_val, top_q=top_q)\n",
    "        preds_all.extend(pred)\n",
    "        rets_all.extend(strat_ret)\n",
    "\n",
    "    if len(rets_all) < 10:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    rets_all = np.array(rets_all)\n",
    "    target_market = ret_market[: len(rets_all)]\n",
    "    shr = modified_sharpe(rets_all, target_market)\n",
    "    return -shr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patched train_stack_and_save function\n",
    "def train_stack_and_save(\n",
    "    train_df,\n",
    "    test_df=None,\n",
    "    outdir='outputs',\n",
    "    n_splits=DEFAULT_N_SPLITS,\n",
    "    purge_days=DEFAULT_PURGE_DAYS,\n",
    "    embargo_days=DEFAULT_EMBARGO_DAYS,\n",
    "    n_trials=DEFAULT_N_TRIALS,\n",
    "    top_k=DEFAULT_TOP_K,\n",
    "    use_gpu=True,\n",
    "):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # enforce chronological order\n",
    "    if TIME_COL in train_df.columns:\n",
    "        train_df = train_df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "    else:\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "    if test_df is not None:\n",
    "        if TIME_COL in test_df.columns:\n",
    "            test_df = test_df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "        else:\n",
    "            test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    # choose price column\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'E1' in train_df.columns:\n",
    "        price_col = 'E1'\n",
    "    else:\n",
    "        candidate = [c for c in numeric_cols if c not in [TIME_COL, TARGET_COL]]\n",
    "        price_col = candidate[0] if candidate else numeric_cols[0]\n",
    "\n",
    "    # feature engineer\n",
    "    train_df = feature_engineer(train_df, price_col)\n",
    "    test_df = feature_engineer(test_df, price_col) if test_df is not None else None\n",
    "\n",
    "    # exclude metadata\n",
    "    exclude = {TIME_COL, TARGET_COL, 'market_forward_excess_returns', 'risk_free_rate', 'date', 'is_scored'}\n",
    "    features = [c for c in train_df.select_dtypes(include=[np.number]).columns if c not in exclude]\n",
    "\n",
    "    # prefer lagged columns in test when present\n",
    "    if test_df is not None:\n",
    "        for col in list(test_df.columns):\n",
    "            if col.startswith('lagged_'):\n",
    "                base = col.replace('lagged_', '')\n",
    "                if base in features and base not in test_df.columns:\n",
    "                    test_df[base] = test_df[col]\n",
    "\n",
    "    # median imputer fit on train only\n",
    "    X_full = train_df[features].copy()\n",
    "    y_full = train_df[TARGET_COL].values\n",
    "    ret_market = train_df['market_forward_excess_returns'].values if 'market_forward_excess_returns' in train_df.columns else y_full\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputer.fit(X_full)\n",
    "    X_full_imp = pd.DataFrame(imputer.transform(X_full), columns=features, index=X_full.index)\n",
    "    if test_df is not None:\n",
    "        for c in features:\n",
    "            if c not in test_df.columns:\n",
    "                test_df[c] = np.nan\n",
    "        test_df[features] = pd.DataFrame(imputer.transform(test_df[features]), columns=features, index=test_df.index)\n",
    "\n",
    "    # feature selection\n",
    "    k = min(top_k, X_full_imp.shape[1])\n",
    "    skb = SelectKBest(score_func=f_regression, k=k)\n",
    "    skb.fit(X_full_imp.values, y_full)\n",
    "    selected_idx = skb.get_support(indices=True)\n",
    "    features = [features[i] for i in selected_idx]\n",
    "\n",
    "    X = X_full_imp[features].values\n",
    "    n = len(X)\n",
    "    splits = list(purged_embargo_splits(n, n_splits=n_splits, purge=purge_days, embargo=embargo_days))\n",
    "\n",
    "    # gpu checks\n",
    "    use_gpu_effective = use_gpu\n",
    "    if use_gpu:\n",
    "        if lgb is None or CatBoostRegressor is None:\n",
    "            print('GPU requested but LGB/CatBoost not available - falling back to CPU.')\n",
    "            use_gpu_effective = False\n",
    "\n",
    "    # optuna\n",
    "    sampler = optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    study = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\n",
    "    print(f\"{datetime.now()} - Starting Optuna tuning ({n_trials} trials)...\")\n",
    "    study.optimize(lambda t: objective(t, X, y_full, ret_market, splits, use_gpu_effective), n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print('Best params:', study.best_params)\n",
    "    print('Best modified sharpe (train):', -study.best_value)\n",
    "    best = study.best_params\n",
    "\n",
    "    # final models\n",
    "    lgb_final_params = {\n",
    "        'num_leaves': best.get('num_leaves', 64),\n",
    "        'learning_rate': best.get('lgb_lr', 0.05),\n",
    "        'n_estimators': best.get('lgb_estimators', 300),\n",
    "        'min_child_samples': best.get('min_child_samples', 20),\n",
    "        'subsample': best.get('subsample', 0.8),\n",
    "        'colsample_bytree': best.get('colsample_bytree', 0.8),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    if use_gpu_effective:\n",
    "        lgb_final_params['device'] = 'gpu'\n",
    "        lgb_final_params['gpu_platform_id'] = 0\n",
    "        lgb_final_params['gpu_device_id'] = 0\n",
    "\n",
    "    final_lgb = lgb.LGBMRegressor(**lgb_final_params)\n",
    "    final_cat_args = {\n",
    "        'iterations': best.get('cat_iters', 600),\n",
    "        'depth': best.get('cat_depth', 6),\n",
    "        'learning_rate': best.get('cat_lr', 0.03),\n",
    "        'verbose': False,\n",
    "        'random_state': RANDOM_STATE,\n",
    "    }\n",
    "    if use_gpu_effective:\n",
    "        final_cat_args['task_type'] = 'GPU'\n",
    "    final_cat = CatBoostRegressor(**final_cat_args)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    final_lgb.fit(X_scaled, y_full)\n",
    "    final_cat.fit(X_scaled, y_full)\n",
    "\n",
    "    # OOF stacking\n",
    "    oof_lgb = np.zeros_like(y_full, dtype=float)\n",
    "    oof_cat = np.zeros_like(y_full, dtype=float)\n",
    "    for train_idx, val_idx in splits:\n",
    "        if len(train_idx) < 10 or len(val_idx) < 10:\n",
    "            continue\n",
    "        Xtr = scaler.transform(X[train_idx])\n",
    "        Xv = scaler.transform(X[val_idx])\n",
    "        ytr = y_full[train_idx]\n",
    "        yv = y_full[val_idx]\n",
    "\n",
    "        lgb_local = lgb.LGBMRegressor(**final_lgb.get_params())\n",
    "        if use_gpu_effective:\n",
    "            try:\n",
    "                lgb_local.set_params(device='gpu', gpu_platform_id=0, gpu_device_id=0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            cat_params = final_cat.get_all_params()\n",
    "            cat_local = CatBoostRegressor(\n",
    "                iterations=int(cat_params.get('iterations', final_cat_args['iterations'])),\n",
    "                depth=int(cat_params.get('depth', final_cat_args['depth'])),\n",
    "                learning_rate=float(cat_params.get('learning_rate', final_cat_args['learning_rate'])),\n",
    "                verbose=False,\n",
    "                random_state=RANDOM_STATE,\n",
    "            )\n",
    "        except Exception:\n",
    "            cat_local = CatBoostRegressor(**final_cat_args)\n",
    "\n",
    "        if use_gpu_effective:\n",
    "            try:\n",
    "                cat_local.set_params(task_type='GPU')\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            lgb_local.fit(Xtr, ytr, eval_set=[(Xv, yv)], early_stopping_rounds=50, verbose=False)\n",
    "        except Exception:\n",
    "            lgb_local.fit(Xtr, ytr)\n",
    "\n",
    "        try:\n",
    "            cat_local.fit(Xtr, ytr, eval_set=(Xv, yv), use_best_model=True, verbose=False)\n",
    "        except Exception:\n",
    "            cat_local.fit(Xtr, ytr, verbose=False)\n",
    "\n",
    "        oof_lgb[val_idx] = lgb_local.predict(Xv)\n",
    "        oof_cat[val_idx] = cat_local.predict(Xv)\n",
    "\n",
    "    meta_X = np.vstack([oof_lgb, oof_cat]).T\n",
    "    meta = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "    meta.fit(meta_X, y_full)\n",
    "\n",
    "    base_pred_full = 0.5 * final_lgb.predict(X_scaled) + 0.5 * final_cat.predict(X_scaled)\n",
    "    final_meta_pred = meta.predict(np.vstack([base_pred_full, final_cat.predict(X_scaled)]).T)\n",
    "    top_q = best.get('top_q', 0.2)\n",
    "    train_strat, _, _ = backtest_with_cost_from_preds(final_meta_pred, y_full, top_q=top_q, tc=DEFAULT_TRANSACTION_COST)\n",
    "    final_sharpe = modified_sharpe(train_strat, ret_market)\n",
    "    print(f'Final full-sample modified Sharpe: {final_sharpe:.4f}')\n",
    "\n",
    "    artifact = {'lgb': final_lgb, 'cat': final_cat, 'meta': meta, 'scaler': scaler, 'features': features, 'best_optuna': best, 'use_gpu': use_gpu_effective, 'train_sharpe': final_sharpe, 'imputer': imputer}\n",
    "    joblib.dump(artifact, os.path.join(outdir, 'models.joblib'))\n",
    "    print('Saved models to', os.path.join(outdir, 'models.joblib'))\n",
    "\n",
    "    if test_df is not None:\n",
    "        X_test = test_df[features].values\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        base_pred_test = 0.5 * final_lgb.predict(X_test_scaled) + 0.5 * final_cat.predict(X_test_scaled)\n",
    "        pred_meta_test = meta.predict(np.vstack([base_pred_test, final_cat.predict(X_test_scaled)]).T)\n",
    "        if TIME_COL in test_df.columns:\n",
    "            sub = test_df[[TIME_COL]].copy()\n",
    "        elif 'id' in test_df.columns:\n",
    "            sub = test_df[['id']].copy()\n",
    "        else:\n",
    "            sub = pd.DataFrame({'id': np.arange(len(test_df))})\n",
    "        sub['prediction'] = pred_meta_test\n",
    "        if 'is_scored' in test_df.columns:\n",
    "            sub.to_csv(os.path.join(outdir, 'submission_full.csv'), index=False)\n",
    "            sub_scored = sub[test_df['is_scored'] == 1].reset_index(drop=True)\n",
    "            sub_scored.to_csv(os.path.join(outdir, 'submission_scored_only.csv'), index=False)\n",
    "            print('Wrote submission_full.csv and submission_scored_only.csv (scored rows only).')\n",
    "        else:\n",
    "            sub.to_csv(os.path.join(outdir, 'submission.csv'), index=False)\n",
    "            print('Wrote submission.csv')\n",
    "\n",
    "    return artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414753a-ab63-4c2a-8227-6ce431732baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Optuna HPO with turnover penalty (heavy) =====\n",
    "import os, time, traceback\n",
    "import optuna\n",
    "import joblib\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# CONFIG - tune these before running\n",
    "OUTDIR = \"outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "study_db = \"sqlite:///optuna_turnover.db\"   # persistent study (can resume)\n",
    "n_trials = 20     # start with 40 for testing; raise to 100-300 for final runs\n",
    "n_splits = 5\n",
    "purge_days = 30\n",
    "embargo_days = 2\n",
    "top_k = 120         # num features selected with univariate filter\n",
    "turnover_penalty_coef = 200.0   # tune this: higher => prefer lower turnover\n",
    "use_gpu_flag = has_nvidia_gpu()  # your helper function from notebook\n",
    "\n",
    "print(\"HPO config:\", dict(n_trials=n_trials, n_splits=n_splits, top_k=top_k, use_gpu=use_gpu_flag))\n",
    "\n",
    "# Prepare training matrix (impute medians & select top_k)\n",
    "exclude = {TIME_COL, TARGET_COL, \"market_forward_excess_returns\", \"risk_free_rate\", \"date\", \"is_scored\"}\n",
    "candidates = [c for c in train_df.select_dtypes(include=[np.number]).columns if c not in exclude]\n",
    "X_full_df = train_df[candidates].copy()\n",
    "y_full = train_df[TARGET_COL].values\n",
    "ret_market_full = train_df[\"market_forward_excess_returns\"].values if \"market_forward_excess_returns\" in train_df.columns else y_full\n",
    "\n",
    "# Impute (train-only)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_full_df)\n",
    "X_full_imp = pd.DataFrame(imputer.transform(X_full_df), columns=candidates, index=X_full_df.index)\n",
    "\n",
    "# Feature pre-selection\n",
    "k_sel = min(top_k, X_full_imp.shape[1])\n",
    "skb = SelectKBest(score_func=f_regression, k=k_sel)\n",
    "skb.fit(X_full_imp.values, y_full)\n",
    "sel_idx = skb.get_support(indices=True)\n",
    "features_opt = [candidates[i] for i in sel_idx]\n",
    "X = X_full_imp[features_opt].values\n",
    "ret_market = ret_market_full\n",
    "\n",
    "print(\"Prepared X with shape:\", X.shape, \"features:\", len(features_opt))\n",
    "\n",
    "# Splits (index-based purged+embargo)\n",
    "def purged_embargo_splits_local(n_samples, n_splits=n_splits, purge=purge_days, embargo=embargo_days):\n",
    "    idx = np.arange(n_samples)\n",
    "    block_size = max(1, n_samples // n_splits)\n",
    "    for i in range(n_splits):\n",
    "        val_start = i * block_size\n",
    "        val_end = min(n_samples, val_start + block_size)\n",
    "        train_end = max(0, val_start - purge)\n",
    "        train_idx = idx[:train_end]\n",
    "        val_idx = idx[val_start:val_end]\n",
    "        yield train_idx, val_idx\n",
    "\n",
    "splits_local = list(purged_embargo_splits_local(len(X), n_splits=n_splits, purge=purge_days, embargo=embargo_days))\n",
    "\n",
    "# Objective with turnover penalty\n",
    "def objective_turnover(trial):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        # LGB params (expanded)\n",
    "        lgb_params = {\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 512),\n",
    "            \"learning_rate\": trial.suggest_float(\"lgb_lr\", 1e-4, 0.2, log=True),\n",
    "            \"n_estimators\": trial.suggest_int(\"lgb_estimators\", 100, 1500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 16),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 200),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "\n",
    "        # Cat params (expanded)\n",
    "        cat_iters = trial.suggest_int(\"cat_iters\", 200, 2000)\n",
    "        cat_depth = trial.suggest_int(\"cat_depth\", 3, 12)\n",
    "        cat_lr = trial.suggest_float(\"cat_lr\", 1e-3, 0.2, log=True)\n",
    "\n",
    "        # signal selection and ensemble weight\n",
    "        top_q = trial.suggest_float(\"top_q\", 0.01, 0.4)\n",
    "        ensemble_w = trial.suggest_float(\"ensemble_w\", 0.0, 1.0)\n",
    "\n",
    "        preds_all = []\n",
    "        rets_all = []\n",
    "        turnovers = []\n",
    "\n",
    "        for train_idx, val_idx in splits_local:\n",
    "            if len(train_idx) < 10 or len(val_idx) < 10:\n",
    "                continue\n",
    "            Xtr, Xv = X[train_idx], X[val_idx]\n",
    "            ytr, yv = y_full[train_idx], y_full[val_idx]\n",
    "\n",
    "            # LightGBM\n",
    "            import lightgbm as lgb_local_mod\n",
    "            lgb_args = deepcopy(lgb_params)\n",
    "            if use_gpu_flag:\n",
    "                # these params may be ignored depending on build\n",
    "                lgb_args[\"device\"] = \"gpu\"\n",
    "                lgb_args[\"gpu_platform_id\"] = 0\n",
    "                lgb_args[\"gpu_device_id\"] = 0\n",
    "            mdl_l = lgb_local_mod.LGBMRegressor(**lgb_args)\n",
    "            try:\n",
    "                mdl_l.fit(Xtr, ytr, eval_set=[(Xv, yv)], early_stopping_rounds=50, verbose=False)\n",
    "            except Exception:\n",
    "                mdl_l.fit(Xtr, ytr)\n",
    "\n",
    "            # CatBoost\n",
    "            from catboost import CatBoostRegressor as CatLocal\n",
    "            cat_args = {\"iterations\": int(cat_iters), \"depth\": int(cat_depth), \"learning_rate\": float(cat_lr), \"verbose\": False, \"random_state\": RANDOM_STATE}\n",
    "            if use_gpu_flag:\n",
    "                cat_args[\"task_type\"] = \"GPU\"\n",
    "            mdl_c = CatLocal(**cat_args)\n",
    "            try:\n",
    "                mdl_c.fit(Xtr, ytr, eval_set=(Xv, yv), use_best_model=True, verbose=False)\n",
    "            except Exception:\n",
    "                mdl_c.fit(Xtr, ytr, verbose=False)\n",
    "\n",
    "            pred_v = ensemble_w * mdl_l.predict(Xv) + (1.0 - ensemble_w) * mdl_c.predict(Xv)\n",
    "            strat_v, pos_v, cost_v = backtest_with_cost_from_preds(pred_v, yv, top_q=top_q, tc=DEFAULT_TRANSACTION_COST)\n",
    "            preds_all.extend(pred_v)\n",
    "            rets_all.extend(strat_v)\n",
    "            turnovers.append(np.abs(np.diff(pos_v, prepend=0)).mean())\n",
    "\n",
    "        if len(rets_all) < 10:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        rets_all = np.array(rets_all)\n",
    "        avg_turn = float(np.mean(turnovers)) if len(turnovers) else 0.0\n",
    "        shr = modified_sharpe(rets_all, ret_market[: len(rets_all)])\n",
    "        # objective: minimize -sharpe + penalty * turnover\n",
    "        objective_value = float(-shr + turnover_penalty_coef * avg_turn)\n",
    "\n",
    "        # log user attrs\n",
    "        trial.set_user_attr(\"sharpe\", float(shr))\n",
    "        trial.set_user_attr(\"avg_turnover\", float(avg_turn))\n",
    "        trial.set_user_attr(\"elapsed\", time.time() - t0)\n",
    "\n",
    "        return objective_value\n",
    "    except Exception:\n",
    "        # log and re-raise so Optuna records the failure\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# Create persistent study so you can resume\n",
    "study = optuna.create_study(storage=study_db, study_name=\"turnover_study\", load_if_exists=True, direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE), pruner=optuna.pruners.MedianPruner())\n",
    "print(\"Starting Optuna (will save to):\", study_db)\n",
    "study.optimize(objective_turnover, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "print(\"Study finished. Best value (objective):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best trial attrs:\", study.best_trial.user_attrs)\n",
    "\n",
    "# Train final models on full data using best params\n",
    "best = study.best_params\n",
    "lgb_final = lgb.LGBMRegressor(\n",
    "    num_leaves=best.get(\"num_leaves\", 64),\n",
    "    learning_rate=best.get(\"lgb_lr\", 0.05),\n",
    "    n_estimators=best.get(\"lgb_estimators\", 300),\n",
    "    max_depth=best.get(\"max_depth\", -1),\n",
    "    min_child_samples=best.get(\"min_child_samples\", 20),\n",
    "    subsample=best.get(\"subsample\", 0.8),\n",
    "    colsample_bytree=best.get(\"colsample_bytree\", 0.8),\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "if use_gpu_flag:\n",
    "    try:\n",
    "        lgb_final.set_params(device=\"gpu\", gpu_platform_id=0, gpu_device_id=0)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "cat_final = CatBoostRegressor(\n",
    "    iterations=int(best.get(\"cat_iters\", 600)),\n",
    "    depth=int(best.get(\"cat_depth\", 6)),\n",
    "    learning_rate=float(best.get(\"cat_lr\", 0.03)),\n",
    "    verbose=False,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "if use_gpu_flag:\n",
    "    try:\n",
    "        cat_final.set_params(task_type=\"GPU\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Fit on full X\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Fitting final LGB on full data...\")\n",
    "try:\n",
    "    lgb_final.fit(X_scaled, y_full, verbose=False)\n",
    "except Exception:\n",
    "    lgb_final.fit(X_scaled, y_full)\n",
    "print(\"Fitting final CatBoost on full data...\")\n",
    "try:\n",
    "    cat_final.fit(X_scaled, y_full, verbose=False)\n",
    "except Exception:\n",
    "    cat_final.fit(X_scaled, y_full)\n",
    "\n",
    "# Build simple ensemble and (optional) meta with small LGB if desired\n",
    "ensemble_w = best.get(\"ensemble_w\", 0.5)\n",
    "base_pred_full = ensemble_w * lgb_final.predict(X_scaled) + (1 - ensemble_w) * cat_final.predict(X_scaled)\n",
    "\n",
    "# Optional: small LGB meta trained on base preds\n",
    "meta_lgb = lgb.LGBMRegressor(num_leaves=31, n_estimators=200, random_state=RANDOM_STATE)\n",
    "meta_lgb.fit(np.vstack([base_pred_full, cat_final.predict(X_scaled)]).T, y_full)\n",
    "\n",
    "# diagnostics\n",
    "final_strat, _, _ = backtest_with_cost_from_preds(meta_lgb.predict(np.vstack([base_pred_full, cat_final.predict(X_scaled)]).T), y_full, top_q=best.get(\"top_q\", 0.2), tc=DEFAULT_TRANSACTION_COST)\n",
    "print(\"Final train modified Sharpe (meta_lgb):\", modified_sharpe(final_strat, ret_market))\n",
    "\n",
    "# Save artifact\n",
    "artifact = {\n",
    "    \"lgb\": lgb_final,\n",
    "    \"cat\": cat_final,\n",
    "    \"meta\": meta_lgb,\n",
    "    \"scaler\": scaler,\n",
    "    \"features\": features_opt,\n",
    "    \"study_db\": study_db,\n",
    "    \"optuna_best\": best,\n",
    "}\n",
    "joblib.dump(artifact, os.path.join(OUTDIR, \"models_optuna_turnover.joblib\"))\n",
    "print(\"Saved final artifact to:\", os.path.join(OUTDIR, \"models_optuna_turnover.joblib\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30344fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "art = joblib.load(os.path.join(OUTDIR, 'models.joblib'))\n",
    "print('Saved artifact keys:', list(art.keys()))\n",
    "print('Train modified Sharpe:', art.get('train_sharpe'))\n",
    "print('Selected features (sample):', art['features'][:20])\n",
    "for f in ['submission_full.csv', 'submission_scored_only.csv', 'submission.csv']:\n",
    "    p = os.path.join(OUTDIR, f)\n",
    "    if os.path.exists(p):\n",
    "        print('\\n', f, '->', p)\n",
    "        display(pd.read_csv(p).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0271215-e2f9-4fdf-b94d-74f33a50efa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-mobipi]",
   "language": "python",
   "name": "conda-env-Anaconda3-mobipi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
